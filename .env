# ─── QSAR LLM — Variables de entorno ───────────────────────────────────────
# Copia este archivo como .env y completa los valores

# URL de Ollama (instalación local)
# Por defecto: http://localhost:11434
OLLAMA_URL=http://localhost:11434

# Modelo de Ollama a usar
# Opciones: mistral, neural-chat, llama2, etc.
OLLAMA_MODEL=mistral

# URL del servidor QSAR Toolbox (instalación local)
# Por defecto: http://localhost:3000
TOOLBOX_URL=http://localhost:3000

# Puerto del servidor Flask
PORT=5001

# Modo debug (true solo en desarrollo)
DEBUG=false

# API key opcional para proteger el backend (deja en blanco para desactivar)
BACKEND_API_KEY=
